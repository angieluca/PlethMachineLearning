{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e4728f",
   "metadata": {},
   "source": [
    "# Final Thesis Project - Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61933fde",
   "metadata": {},
   "source": [
    "This Notebook tests out different ML models and check the scores. \n",
    "\n",
    "The training dataset contains a total of ? samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "678a5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d406923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edward.luca\\Github\\PlethMachineLearning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40b2aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342, 19) (342,)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "data_train = np.load('data_train.npy', allow_pickle=True)\n",
    "labels_train = np.load('labels_train.npy', allow_pickle=True)\n",
    "\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa72a17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Loading Data\\nnapoli_test = np.load('napoli_data.npy', allow_pickle=True)\\nlabels_test = np.load('napoli_labels.npy', allow_pickle=True)\\n\\n#print(da_train.shape, labels_train.shape)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Loading Data\n",
    "napoli_test = np.load('napoli_data.npy', allow_pickle=True)\n",
    "labels_test = np.load('napoli_labels.npy', allow_pickle=True)\n",
    "\n",
    "#print(da_train.shape, labels_train.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fd1a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Encoding\n",
    "\n",
    "labels_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60398e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting number samples per class\n",
    "vals, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(vals, counts)\n",
    "plt.xticks(range(7),range(7))\n",
    "plt.xlabel('Classes',size=20)\n",
    "plt.ylabel('# Samples per Class', size=20)\n",
    "plt.title('Training Data (Total = '+str(data_train.shape[0])+' samples)',size=15);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2985ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from skimage.transform import resize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9908b152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 19)\n",
      "(273,)\n",
      "(69, 19)\n",
      "(69,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(data_train, labels_train, \n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=labels_train,\n",
    "                                                   random_state=0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f58a13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_napoli_train, X_napoli_test, t_napoli_train, t_napoli_test = train_test_split(napoli_test, labels_test, \\n                                                   test_size=0.5,\\n                                                   random_state=0)\\nprint(X_train.shape)\\nprint(t_train.shape)\\nprint(X_test.shape)\\nprint(t_test.shape)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_napoli_train, X_napoli_test, t_napoli_train, t_napoli_test = train_test_split(napoli_test, labels_test, \n",
    "                                                   test_size=0.5,\n",
    "                                                   random_state=0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1a6b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a00b6",
   "metadata": {},
   "source": [
    "## 1.) LDA + LOGISTIC REGRESSION (Model No.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e9e360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()),\n",
       "                ('LDA', LinearDiscriminantAnalysis(n_components=3)),\n",
       "                ('LOGRES', LogisticRegression())])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('LDA', LDA(n_components=3)),\n",
    "                 ('LOGRES', LogisticRegression())])\n",
    "mod1.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06d4b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################   RUN GRIDSEARCHCV ON ALL    ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6175ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1 = mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db7cb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test1 = mod1.predict(X_napoli_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed59afda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA + LR:\n",
      "Training Accuracy: \n",
      "  0.8985507246376812\n",
      "F1_score:\n",
      " [0.82051282 1.         0.92307692 0.86666667]\n",
      "Confusion matrix:\n",
      "[[16  0  1  3]\n",
      " [ 0 15  0  0]\n",
      " [ 2  0 18  0]\n",
      " [ 1  0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "print('LDA + LR:')\n",
    "print('Training Accuracy: \\n ',accuracy_score(t_test, pred_test1))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test1, average=None))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aea5d564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('LR\\n')\\nprint('Accuracy:\\n',accuracy_score(t_napoli_test, pred_test1))\\nprint ('F1_score:\\n',f1_score(t_napoli_test, pred_test1, average=None))\\nprint('Confusion matrix:\\n',confusion_matrix(t_napoli_test, pred_test1))\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('LR\\n')\n",
    "print('Accuracy:\\n',accuracy_score(t_napoli_test, pred_test1))\n",
    "print ('F1_score:\\n',f1_score(t_napoli_test, pred_test1, average=None))\n",
    "print('Confusion matrix:\\n',confusion_matrix(t_napoli_test, pred_test1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad403fc4",
   "metadata": {},
   "source": [
    "## 2.) PCA + LOGISTIC REGRESSION (Model No. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2bc3fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18], dtype=int64),)\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, D = np.shape(X_train)\n",
    "pca = PCA(n_components=min(N,D))\n",
    "pca.fit(X_train)\n",
    "\n",
    "plt.step(range(1,min(N,D)+1),np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "print(np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9))\n",
    "print(np.cumsum(pca.explained_variance_ratio_)[18])\n",
    "plt.xlabel('Number of principal components');\n",
    "plt.ylabel('% Variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6032fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('PCA', PCA(n_components=19)),\n",
    "                 ('LOGREG', LogisticRegression(random_state=0, tol=0.01))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a343ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()), ('PCA', PCA(n_components=19)),\n",
       "                ('LOGREG', LogisticRegression(random_state=0, tol=0.01))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6433842d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA + LR:\n",
      "Training Accuracy: \n",
      "  0.9420289855072463\n",
      "F1_score:\n",
      " [0.92307692 0.96551724 0.95       0.93333333]\n",
      "Confusion matrix:\n",
      "[[18  0  1  1]\n",
      " [ 0 14  0  1]\n",
      " [ 1  0 19  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "pred_test2 = mod2.predict(X_test)\n",
    "\n",
    "print('PCA + LR:')\n",
    "print('Training Accuracy: \\n ',accuracy_score(t_test, pred_test2))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test2, average=None))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9f9257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npred_test2 = mod2.predict(X_napoli_test)\\n\\nprint('Accuracy:\\n',accuracy_score(t_napoli_test, pred_test2))\\nprint ('F1_score:\\n',f1_score(t_napoli_test, pred_test2, average=None))\\nprint('Confusion matrix:\\n',confusion_matrix(t_napoli_test, pred_test2))\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pred_test2 = mod2.predict(X_napoli_test)\n",
    "\n",
    "print('Accuracy:\\n',accuracy_score(t_napoli_test, pred_test2))\n",
    "print ('F1_score:\\n',f1_score(t_napoli_test, pred_test2, average=None))\n",
    "print('Confusion matrix:\\n',confusion_matrix(t_napoli_test, pred_test2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37379e40",
   "metadata": {},
   "source": [
    "## 3.) Random Forest (Model No. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffcf2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23d76ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5de1c419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2aa84c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF:\n",
      "Training Accuracy: \n",
      "  0.9710144927536232\n",
      "F1_score:\n",
      " [0.94736842 1.         0.97560976 0.96551724]\n",
      "Confusion matrix:\n",
      "[[18  0  1  1]\n",
      " [ 0 15  0  0]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "pred_test3 = rf_classifier.predict(X_test)\n",
    "\n",
    "print('RF:')\n",
    "print('Training Accuracy: \\n ',accuracy_score(t_test, pred_test3))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test3, average=None))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b5f7b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npred_test3 = rf_classifier.predict(X_napoli_test)\\n\\nprint('With Random Forest:')\\nprint('Test Accuracy Score = ',accuracy_score(t_napoli_test, pred_test3))\\nprint('Confusion matrix:')\\nprint(confusion_matrix(t_napoli_test, pred_test3))\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pred_test3 = rf_classifier.predict(X_napoli_test)\n",
    "\n",
    "print('With Random Forest:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_napoli_test, pred_test3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_napoli_test, pred_test3))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a8cee",
   "metadata": {},
   "source": [
    "## 4.) XGBoost (Model No.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4efcae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7dc2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10218ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_class=5,\n",
       "              num_parallel_tree=None, objective='multi:softmax', ...)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "xgb_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58f0db48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB:\n",
      "Training Accuracy: \n",
      "  0.9710144927536232\n",
      "F1_score:\n",
      " [0.97435897 0.96551724 1.         0.93333333]\n",
      "Confusion matrix:\n",
      "[[19  0  0  1]\n",
      " [ 0 14  0  1]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "pred_test4 = xgb_classifier.predict(X_test)\n",
    "\n",
    "print('XGB:')\n",
    "print('Training Accuracy: \\n ',accuracy_score(t_test, pred_test4))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test4, average=None))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8117df",
   "metadata": {},
   "source": [
    "## 5.) CNN (Model No. 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b916f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9c0d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d874a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_full, t_train_full\n",
    "# free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3a0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34647091",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=[300,300,3]), \n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'), \n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d55bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "127/127 [==============================] - 329s 3s/step - loss: 7.6946 - accuracy: 0.1004 - val_loss: 2.3032 - val_accuracy: 0.0991\n",
      "Epoch 2/2\n",
      "127/127 [==============================] - 324s 3s/step - loss: 2.3043 - accuracy: 0.0910 - val_loss: 2.3029 - val_accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c374bc5970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train, epochs=2, batch_size=32,\n",
    "          validation_data=(X_val, t_val),\n",
    "         callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e47d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 13s 477ms/step - loss: 2.3029 - accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3029372692108154, 0.10112359374761581]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c0b96",
   "metadata": {},
   "source": [
    "## 6) Pre-trained CNN Model Using ResNet without Regularization (Model No. 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59820b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbc417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4969d9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f6e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac15fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4f901",
   "metadata": {},
   "source": [
    "nn.Linear(numFeatures, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, len(trainDS.classes))\n",
    "        # nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27c4b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d35dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb37d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = base_model(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d8ec7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f14cfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 51200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5448a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dense classifier with 10 units and softmax activation function\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82b33761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "777bfdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]),\n",
       " TensorShape([None, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3f5c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "127/127 [==============================] - 112s 842ms/step - loss: 643.4494 - accuracy: 0.3265 - val_loss: 1053.8110 - val_accuracy: 0.2319\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 111s 877ms/step - loss: 403.5866 - accuracy: 0.4958 - val_loss: 1754.6864 - val_accuracy: 0.1903\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 107s 841ms/step - loss: 329.6902 - accuracy: 0.5885 - val_loss: 927.3879 - val_accuracy: 0.2963\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 236.7023 - accuracy: 0.6490 - val_loss: 561.5453 - val_accuracy: 0.4281\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 193.1102 - accuracy: 0.7035 - val_loss: 3759.5913 - val_accuracy: 0.1308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c49bc9f850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_reshaped,t_train, epochs=5, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "181756a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 20s 707ms/step - loss: 3681.6174 - accuracy: 0.1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3681.617431640625, 0.13370786607265472]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d6d90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 21s 713ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4ec9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       1.00      0.02      0.04        90\n",
      "       Adidas       0.00      0.00      0.00        88\n",
      "         Ford       0.86      0.07      0.13        88\n",
      "        Honda       0.67      0.09      0.16        88\n",
      "General Mills       0.00      0.00      0.00        90\n",
      "     Unilever       1.00      0.01      0.02        91\n",
      "   McDonald's       0.10      1.00      0.19        88\n",
      "          KFC       1.00      0.02      0.04        88\n",
      "       Gators       0.86      0.14      0.24        88\n",
      "           3M       0.00      0.00      0.00        91\n",
      "\n",
      "     accuracy                           0.13       890\n",
      "    macro avg       0.55      0.14      0.08       890\n",
      " weighted avg       0.55      0.13      0.08       890\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ea3e7",
   "metadata": {},
   "source": [
    "## 7) Pre-trained CNN Model Using ResNet with Regularization (Model No.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a5c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.utils.set_random_seed(\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20116c06",
   "metadata": {},
   "source": [
    "###  Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1a266",
   "metadata": {},
   "source": [
    "Tutorial on Data Augmentation:\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09df8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, label):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title(label)\n",
    "  plt.imshow(image/255.0)\n",
    "    \n",
    "    \n",
    "\n",
    "def visualize_both(original, augmented):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title('Original image')\n",
    "  plt.imshow(original)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.title('Augmented image')\n",
    "  plt.imshow(augmented/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d5135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_train.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648e235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  keras.layers.RandomFlip(\"horizontal\"),\n",
    "  keras.layers.RandomRotation(0.2),\n",
    "  keras.layers.RandomBrightness(0.3),\n",
    "  keras.layers.RandomContrast(0.4),\n",
    "  #keras.layers.RandomCrop(height=0.5,width=0.5,seed=0),\n",
    "  keras.layers.RandomZoom(height_factor=0.5,width_factor=0.5,seed=0),\n",
    "  #keras.layers.RandomWidth(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e956ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sample = X_train_reshaped\n",
    "t_train_sample = t_train\n",
    "\n",
    "#X_train_sample = X_train_reshaped[0,:,:,:]\n",
    "#t_train_sample = t_train[0]\n",
    "\n",
    "t_train_append = np.append(t_train_sample,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "\n",
    "t_train_append.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1642688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(t_train_append.shape[0]):\n",
    "#    print(t_train_append[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eaa246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_augmented_dataset(dataset):\n",
    "    augmented_dataset =data_augmentation(dataset)\n",
    "    augmented_dataset_numpy = augmented_dataset.numpy()\n",
    "    return augmented_dataset_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b52ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset1 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab74f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000018D6240D0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000018D61921E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset2 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a775d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset3 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a4e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset4 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d8de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(X_train_reshaped,augmented_dataset1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35cb3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2755a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93be1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "757ef81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170, 300, 300, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f12d85",
   "metadata": {},
   "source": [
    "# Importing / Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fb89a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "X_train_augmented = np.load('X_train_augmented.npy')\n",
    "t_train_augmented = np.load('t_train_augmented.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39dd0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffea008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588f6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0796b9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5, 5, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8bcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f7ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac19e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = tf.keras.Sequential()\n",
    "model_seq.add(keras.layers.Dropout(0.25))\n",
    "model_seq.add(base_model)\n",
    "\n",
    "#model_seq.add(keras.layers.Flatten())\n",
    "model_seq.add(keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(512, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(256, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "#model_seq.add(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd0dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = model_seq(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1e7468",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Option 1: Pooling\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert features of shape `base_model.output_shape[1:]` to vectors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_pooling \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m x_pooling\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# Option 1: Pooling\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_pooling = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6e4cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b2a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95bdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "631/631 [==============================] - 432s 678ms/step - loss: 0.5593 - accuracy: 0.8177 - val_loss: 0.2841 - val_accuracy: 0.9118\n",
      "Epoch 2/15\n",
      "631/631 [==============================] - 401s 635ms/step - loss: 0.2630 - accuracy: 0.9097 - val_loss: 0.3734 - val_accuracy: 0.8989\n",
      "Epoch 3/15\n",
      "631/631 [==============================] - 406s 643ms/step - loss: 0.1777 - accuracy: 0.9411 - val_loss: 0.1823 - val_accuracy: 0.9504\n",
      "Epoch 4/15\n",
      "631/631 [==============================] - 397s 629ms/step - loss: 0.1200 - accuracy: 0.9595 - val_loss: 0.2792 - val_accuracy: 0.9405\n",
      "Epoch 5/15\n",
      "631/631 [==============================] - 403s 639ms/step - loss: 0.0945 - accuracy: 0.9682 - val_loss: 0.2525 - val_accuracy: 0.9485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaced3f880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_augmented,t_train_augmented, epochs=15, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f9f361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 16s 577ms/step - loss: 0.1197 - accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11972300708293915, 0.9674157500267029]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037fe0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 18s 601ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c4007b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       0.96      0.98      0.97        90\n",
      "       Adidas       0.99      0.95      0.97        88\n",
      "         Ford       0.99      0.98      0.98        88\n",
      "        Honda       0.98      0.99      0.98        88\n",
      "General_mills       0.98      0.98      0.98        90\n",
      "     Unilever       0.96      1.00      0.98        91\n",
      "    Mcdonalds       0.98      0.94      0.96        88\n",
      "          KFC       0.99      0.92      0.95        88\n",
      "       Gators       0.91      0.97      0.94        88\n",
      "           3M       0.96      0.97      0.96        91\n",
      "\n",
      "     accuracy                           0.97       890\n",
      "    macro avg       0.97      0.97      0.97       890\n",
      " weighted avg       0.97      0.97      0.97       890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68a6713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a32e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
