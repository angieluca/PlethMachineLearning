{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thesis Project - Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook tests out different ML models and check the scores. \n",
    "\n",
    "The training dataset contains a total of ? samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/SuperPawn/Documents/GitHub/PlethMachineLearning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13289, 27) (13289,)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "data_train = np.load('data_train.npy', allow_pickle=True)\n",
    "labels_train = np.load('labels_train.npy', allow_pickle=True)\n",
    "\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Encoding\n",
    "\n",
    "labels_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting number samples per class\n",
    "vals, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(vals, counts)\n",
    "plt.xticks(range(6),range(6))\n",
    "plt.xlabel('Classes',size=20)\n",
    "plt.ylabel('# Samples per Class', size=20)\n",
    "plt.title('Training Data (Total = '+str(data_train.shape[0])+' samples)',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from skimage.transform import resize\n",
    "from sklearn.svm import SVC\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10631, 27)\n",
      "(10631,)\n",
      "(2658, 27)\n",
      "(2658,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(data_train, labels_train, \n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=labels_train,\n",
    "                                                   random_state=0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) LDA + LOGISTIC REGRESSION (Model No.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()),\n",
       "                ('LDA', LinearDiscriminantAnalysis(n_components=4)),\n",
       "                ('LOGRES', LogisticRegression())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('LDA', LDA(n_components=4)),\n",
    "                 ('LOGRES', LogisticRegression())])\n",
    "mod1.fit(X_train, t_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1 = mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\n",
      "Accuracy:\n",
      " 0.8946576373212942\n",
      "F1_score:\n",
      " [0.890625   0.83885542 0.         0.91438212 0.94560669]\n",
      "Confusion matrix:\n",
      " [[ 285   31    0    5    0]\n",
      " [  31  557    1   90    0]\n",
      " [   0    0    0    5   13]\n",
      " [   3   61    1 1084   31]\n",
      " [   0    0    1    7  452]]\n"
     ]
    }
   ],
   "source": [
    "print('LR\\n')\n",
    "print('Accuracy:\\n',accuracy_score(t_test, pred_test1))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test1, average=None))\n",
    "print('Confusion matrix:\\n',confusion_matrix(t_test, pred_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) PCA + LOGISTIC REGRESSION (Model No. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26]),)\n",
      "0.9999999999993558\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, D = np.shape(X_train)\n",
    "pca = PCA(n_components=min(N,D))\n",
    "pca.fit(X_train)\n",
    "\n",
    "plt.step(range(1,min(N,D)+1),np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "print(np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9))\n",
    "print(np.cumsum(pca.explained_variance_ratio_)[20])\n",
    "plt.xlabel('Number of principal components');\n",
    "plt.ylabel('% Variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('PCA', PCA(n_components=26)),\n",
    "                 ('LOGREG', LogisticRegression(random_state=0, tol=0.01))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SuperPawn/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()), ('PCA', PCA(n_components=26)),\n",
       "                ('LOGREG', LogisticRegression(random_state=0, tol=0.01))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With PCA:\n",
      "Test Accuracy Score =  0.8935289691497367\n",
      "Confusion matrix:\n",
      "[[ 282   25    0    9    5]\n",
      " [   5  581    0   93    0]\n",
      " [   0    2    0    2   14]\n",
      " [   1   62    0 1070   47]\n",
      " [   0    0    0   18  442]]\n"
     ]
    }
   ],
   "source": [
    "pred_test2 = mod2.predict(X_test)\n",
    "\n",
    "print('With PCA:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test2))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Random Forest (Model No. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Random Forest:\n",
      "Test Accuracy Score =  0.9981188863807374\n",
      "Confusion matrix:\n",
      "[[ 320    0    0    1    0]\n",
      " [   1  678    0    0    0]\n",
      " [   0    0   18    0    0]\n",
      " [   0    0    2 1178    0]\n",
      " [   0    0    1    0  459]]\n"
     ]
    }
   ],
   "source": [
    "pred_test3 = rf_classifier.predict(X_test)\n",
    "\n",
    "print('With Random Forest:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) XGBoost (Model No.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SuperPawn/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_class=4, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "xgb_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With XGBoost:\n",
      "Test Accuracy Score =  0.9981188863807374\n",
      "Confusion matrix:\n",
      "[[ 320    0    0    1    0]\n",
      " [   2  677    0    0    0]\n",
      " [   1    0   17    0    0]\n",
      " [   0    0    1 1179    0]\n",
      " [   0    0    0    0  460]]\n"
     ]
    }
   ],
   "source": [
    "pred_test4 = xgb_classifier.predict(X_test)\n",
    "\n",
    "print('With XGBoost:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test4))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) CNN (Model No. 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3d1e6d42ad48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_full, t_train_full\n",
    "# free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=[300,300,3]), \n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'), \n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "127/127 [==============================] - 329s 3s/step - loss: 7.6946 - accuracy: 0.1004 - val_loss: 2.3032 - val_accuracy: 0.0991\n",
      "Epoch 2/2\n",
      "127/127 [==============================] - 324s 3s/step - loss: 2.3043 - accuracy: 0.0910 - val_loss: 2.3029 - val_accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c374bc5970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train, epochs=2, batch_size=32,\n",
    "          validation_data=(X_val, t_val),\n",
    "         callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 13s 477ms/step - loss: 2.3029 - accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3029372692108154, 0.10112359374761581]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Pre-trained CNN Model Using ResNet without Regularization (Model No. 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear(numFeatures, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, len(trainDS.classes))\n",
    "        # nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = base_model(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 51200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dense classifier with 10 units and softmax activation function\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]),\n",
       " TensorShape([None, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "127/127 [==============================] - 112s 842ms/step - loss: 643.4494 - accuracy: 0.3265 - val_loss: 1053.8110 - val_accuracy: 0.2319\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 111s 877ms/step - loss: 403.5866 - accuracy: 0.4958 - val_loss: 1754.6864 - val_accuracy: 0.1903\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 107s 841ms/step - loss: 329.6902 - accuracy: 0.5885 - val_loss: 927.3879 - val_accuracy: 0.2963\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 236.7023 - accuracy: 0.6490 - val_loss: 561.5453 - val_accuracy: 0.4281\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 193.1102 - accuracy: 0.7035 - val_loss: 3759.5913 - val_accuracy: 0.1308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c49bc9f850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_reshaped,t_train, epochs=5, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 20s 707ms/step - loss: 3681.6174 - accuracy: 0.1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3681.617431640625, 0.13370786607265472]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 21s 713ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       1.00      0.02      0.04        90\n",
      "       Adidas       0.00      0.00      0.00        88\n",
      "         Ford       0.86      0.07      0.13        88\n",
      "        Honda       0.67      0.09      0.16        88\n",
      "General Mills       0.00      0.00      0.00        90\n",
      "     Unilever       1.00      0.01      0.02        91\n",
      "   McDonald's       0.10      1.00      0.19        88\n",
      "          KFC       1.00      0.02      0.04        88\n",
      "       Gators       0.86      0.14      0.24        88\n",
      "           3M       0.00      0.00      0.00        91\n",
      "\n",
      "     accuracy                           0.13       890\n",
      "    macro avg       0.55      0.14      0.08       890\n",
      " weighted avg       0.55      0.13      0.08       890\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Pre-trained CNN Model Using ResNet with Regularization (Model No.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.utils.set_random_seed(\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial on Data Augmentation:\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, label):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title(label)\n",
    "  plt.imshow(image/255.0)\n",
    "    \n",
    "    \n",
    "\n",
    "def visualize_both(original, augmented):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title('Original image')\n",
    "  plt.imshow(original)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.title('Augmented image')\n",
    "  plt.imshow(augmented/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_train.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  keras.layers.RandomFlip(\"horizontal\"),\n",
    "  keras.layers.RandomRotation(0.2),\n",
    "  keras.layers.RandomBrightness(0.3),\n",
    "  keras.layers.RandomContrast(0.4),\n",
    "  #keras.layers.RandomCrop(height=0.5,width=0.5,seed=0),\n",
    "  keras.layers.RandomZoom(height_factor=0.5,width_factor=0.5,seed=0),\n",
    "  #keras.layers.RandomWidth(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sample = X_train_reshaped\n",
    "t_train_sample = t_train\n",
    "\n",
    "#X_train_sample = X_train_reshaped[0,:,:,:]\n",
    "#t_train_sample = t_train[0]\n",
    "\n",
    "t_train_append = np.append(t_train_sample,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "\n",
    "t_train_append.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(t_train_append.shape[0]):\n",
    "#    print(t_train_append[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_augmented_dataset(dataset):\n",
    "    augmented_dataset =data_augmentation(dataset)\n",
    "    augmented_dataset_numpy = augmented_dataset.numpy()\n",
    "    return augmented_dataset_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset1 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000018D6240D0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000018D61921E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset2 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset3 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset4 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(X_train_reshaped,augmented_dataset1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170, 300, 300, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing / Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "X_train_augmented = np.load('X_train_augmented.npy')\n",
    "t_train_augmented = np.load('t_train_augmented.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5, 5, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = tf.keras.Sequential()\n",
    "model_seq.add(keras.layers.Dropout(0.25))\n",
    "model_seq.add(base_model)\n",
    "\n",
    "#model_seq.add(keras.layers.Flatten())\n",
    "model_seq.add(keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(512, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(256, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "#model_seq.add(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = model_seq(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Option 1: Pooling\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert features of shape `base_model.output_shape[1:]` to vectors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_pooling \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m x_pooling\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# Option 1: Pooling\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_pooling = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "631/631 [==============================] - 432s 678ms/step - loss: 0.5593 - accuracy: 0.8177 - val_loss: 0.2841 - val_accuracy: 0.9118\n",
      "Epoch 2/15\n",
      "631/631 [==============================] - 401s 635ms/step - loss: 0.2630 - accuracy: 0.9097 - val_loss: 0.3734 - val_accuracy: 0.8989\n",
      "Epoch 3/15\n",
      "631/631 [==============================] - 406s 643ms/step - loss: 0.1777 - accuracy: 0.9411 - val_loss: 0.1823 - val_accuracy: 0.9504\n",
      "Epoch 4/15\n",
      "631/631 [==============================] - 397s 629ms/step - loss: 0.1200 - accuracy: 0.9595 - val_loss: 0.2792 - val_accuracy: 0.9405\n",
      "Epoch 5/15\n",
      "631/631 [==============================] - 403s 639ms/step - loss: 0.0945 - accuracy: 0.9682 - val_loss: 0.2525 - val_accuracy: 0.9485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaced3f880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_augmented,t_train_augmented, epochs=15, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 16s 577ms/step - loss: 0.1197 - accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11972300708293915, 0.9674157500267029]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 18s 601ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       0.96      0.98      0.97        90\n",
      "       Adidas       0.99      0.95      0.97        88\n",
      "         Ford       0.99      0.98      0.98        88\n",
      "        Honda       0.98      0.99      0.98        88\n",
      "General_mills       0.98      0.98      0.98        90\n",
      "     Unilever       0.96      1.00      0.98        91\n",
      "    Mcdonalds       0.98      0.94      0.96        88\n",
      "          KFC       0.99      0.92      0.95        88\n",
      "       Gators       0.91      0.97      0.94        88\n",
      "           3M       0.96      0.97      0.96        91\n",
      "\n",
      "     accuracy                           0.97       890\n",
      "    macro avg       0.97      0.97      0.97       890\n",
      " weighted avg       0.97      0.97      0.97       890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
