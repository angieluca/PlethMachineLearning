{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bd0f13",
   "metadata": {},
   "source": [
    "# Final Thesis Project - Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837813a4",
   "metadata": {},
   "source": [
    "This Notebook tests out different ML models and check the scores. \n",
    "\n",
    "The training dataset contains a total of ? samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae16b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e246e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edward.luca\\Github\\PlethMachineLearning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5423a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342, 27) (342,)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "data_train = np.load('data_train.npy', allow_pickle=True)\n",
    "labels_train = np.load('labels_train.npy', allow_pickle=True)\n",
    "\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00e819c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Encoding\n",
    "\n",
    "labels_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf4e55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting number samples per class\n",
    "vals, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(vals, counts)\n",
    "plt.xticks(range(7),range(7))\n",
    "plt.xlabel('Classes',size=20)\n",
    "plt.ylabel('# Samples per Class', size=20)\n",
    "plt.title('Training Data (Total = '+str(data_train.shape[0])+' samples)',size=15);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c11a0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from skimage.transform import resize\n",
    "from sklearn.svm import SVC\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc8d9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 27)\n",
      "(273,)\n",
      "(69, 27)\n",
      "(69,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(data_train, labels_train, \n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=labels_train,\n",
    "                                                   random_state=0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078ad78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f192e",
   "metadata": {},
   "source": [
    "## 1.) LDA + LOGISTIC REGRESSION (Model No.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a45a268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()),\n",
       "                ('LDA', LinearDiscriminantAnalysis(n_components=3)),\n",
       "                ('LOGRES', LogisticRegression())])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('LDA', LDA(n_components=3)),\n",
    "                 ('LOGRES', LogisticRegression())])\n",
    "mod1.fit(X_train, t_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0bb31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################   RUN GRIDSEARCHCV ON ALL    ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "880dc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1 = mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e0e1025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\n",
      "Accuracy:\n",
      " 0.9710144927536232\n",
      "F1_score:\n",
      " [0.95       1.         0.97435897 0.96551724]\n",
      "Confusion matrix:\n",
      " [[19  0  0  1]\n",
      " [ 0 15  0  0]\n",
      " [ 1  0 19  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "print('LR\\n')\n",
    "print('Accuracy:\\n',accuracy_score(t_test, pred_test1))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test1, average=None))\n",
    "print('Confusion matrix:\\n',confusion_matrix(t_test, pred_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2138975",
   "metadata": {},
   "source": [
    "## 2.) PCA + LOGISTIC REGRESSION (Model No. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57db1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64),)\n",
      "0.9999999983446723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, D = np.shape(X_train)\n",
    "pca = PCA(n_components=min(N,D))\n",
    "pca.fit(X_train)\n",
    "\n",
    "plt.step(range(1,min(N,D)+1),np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "print(np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9))\n",
    "print(np.cumsum(pca.explained_variance_ratio_)[20])\n",
    "plt.xlabel('Number of principal components');\n",
    "plt.ylabel('% Variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "771eb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('PCA', PCA(n_components=26)),\n",
    "                 ('LOGREG', LogisticRegression(random_state=0, tol=0.01))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e30adac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()), ('PCA', PCA(n_components=26)),\n",
       "                ('LOGREG', LogisticRegression(random_state=0, tol=0.01))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5997c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With PCA:\n",
      "Test Accuracy Score =  0.9710144927536232\n",
      "Confusion matrix:\n",
      "[[19  0  0  1]\n",
      " [ 0 15  0  0]\n",
      " [ 1  0 19  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "pred_test2 = mod2.predict(X_test)\n",
    "\n",
    "print('With PCA:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test2))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf63d4",
   "metadata": {},
   "source": [
    "## 3.) Random Forest (Model No. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96d311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5624b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49ed5472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "819bf8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Random Forest:\n",
      "Test Accuracy Score =  0.9855072463768116\n",
      "Confusion matrix:\n",
      "[[19  0  0  1]\n",
      " [ 0 15  0  0]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "pred_test3 = rf_classifier.predict(X_test)\n",
    "\n",
    "print('With Random Forest:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a5db9",
   "metadata": {},
   "source": [
    "## 4.) XGBoost (Model No.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a654a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8a8000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce01505a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_class=5,\n",
       "              num_parallel_tree=None, objective='multi:softmax', ...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "xgb_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0eeb1e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With XGBoost:\n",
      "Test Accuracy Score =  1.0\n",
      "Confusion matrix:\n",
      "[[20  0  0  0]\n",
      " [ 0 15  0  0]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "pred_test4 = xgb_classifier.predict(X_test)\n",
    "\n",
    "print('With XGBoost:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test4))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd81d4",
   "metadata": {},
   "source": [
    "## 5.) CNN (Model No. 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b124395f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3d1e6d42ad48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f70bf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e49996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39d4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_full, t_train_full\n",
    "# free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017d5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a817652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=[300,300,3]), \n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'), \n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb48668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "127/127 [==============================] - 329s 3s/step - loss: 7.6946 - accuracy: 0.1004 - val_loss: 2.3032 - val_accuracy: 0.0991\n",
      "Epoch 2/2\n",
      "127/127 [==============================] - 324s 3s/step - loss: 2.3043 - accuracy: 0.0910 - val_loss: 2.3029 - val_accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c374bc5970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train, epochs=2, batch_size=32,\n",
    "          validation_data=(X_val, t_val),\n",
    "         callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71e240b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 13s 477ms/step - loss: 2.3029 - accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3029372692108154, 0.10112359374761581]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d920484",
   "metadata": {},
   "source": [
    "## 6) Pre-trained CNN Model Using ResNet without Regularization (Model No. 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9edc452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f1ebde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0607fcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477fc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24905076",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45460de0",
   "metadata": {},
   "source": [
    "nn.Linear(numFeatures, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, len(trainDS.classes))\n",
    "        # nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff85d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a794e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f1c5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = base_model(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ef19353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f16ecefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 51200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e424e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dense classifier with 10 units and softmax activation function\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3660963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f49b217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]),\n",
       " TensorShape([None, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55cad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "127/127 [==============================] - 112s 842ms/step - loss: 643.4494 - accuracy: 0.3265 - val_loss: 1053.8110 - val_accuracy: 0.2319\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 111s 877ms/step - loss: 403.5866 - accuracy: 0.4958 - val_loss: 1754.6864 - val_accuracy: 0.1903\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 107s 841ms/step - loss: 329.6902 - accuracy: 0.5885 - val_loss: 927.3879 - val_accuracy: 0.2963\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 236.7023 - accuracy: 0.6490 - val_loss: 561.5453 - val_accuracy: 0.4281\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 193.1102 - accuracy: 0.7035 - val_loss: 3759.5913 - val_accuracy: 0.1308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c49bc9f850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_reshaped,t_train, epochs=5, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "facf5d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 20s 707ms/step - loss: 3681.6174 - accuracy: 0.1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3681.617431640625, 0.13370786607265472]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa4fc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 21s 713ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a0e9153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       1.00      0.02      0.04        90\n",
      "       Adidas       0.00      0.00      0.00        88\n",
      "         Ford       0.86      0.07      0.13        88\n",
      "        Honda       0.67      0.09      0.16        88\n",
      "General Mills       0.00      0.00      0.00        90\n",
      "     Unilever       1.00      0.01      0.02        91\n",
      "   McDonald's       0.10      1.00      0.19        88\n",
      "          KFC       1.00      0.02      0.04        88\n",
      "       Gators       0.86      0.14      0.24        88\n",
      "           3M       0.00      0.00      0.00        91\n",
      "\n",
      "     accuracy                           0.13       890\n",
      "    macro avg       0.55      0.14      0.08       890\n",
      " weighted avg       0.55      0.13      0.08       890\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca67ff1",
   "metadata": {},
   "source": [
    "## 7) Pre-trained CNN Model Using ResNet with Regularization (Model No.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace4417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.utils.set_random_seed(\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746adea",
   "metadata": {},
   "source": [
    "###  Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb2935",
   "metadata": {},
   "source": [
    "Tutorial on Data Augmentation:\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d9dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, label):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title(label)\n",
    "  plt.imshow(image/255.0)\n",
    "    \n",
    "    \n",
    "\n",
    "def visualize_both(original, augmented):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title('Original image')\n",
    "  plt.imshow(original)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.title('Augmented image')\n",
    "  plt.imshow(augmented/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d61d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_train.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951d4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  keras.layers.RandomFlip(\"horizontal\"),\n",
    "  keras.layers.RandomRotation(0.2),\n",
    "  keras.layers.RandomBrightness(0.3),\n",
    "  keras.layers.RandomContrast(0.4),\n",
    "  #keras.layers.RandomCrop(height=0.5,width=0.5,seed=0),\n",
    "  keras.layers.RandomZoom(height_factor=0.5,width_factor=0.5,seed=0),\n",
    "  #keras.layers.RandomWidth(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135deedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sample = X_train_reshaped\n",
    "t_train_sample = t_train\n",
    "\n",
    "#X_train_sample = X_train_reshaped[0,:,:,:]\n",
    "#t_train_sample = t_train[0]\n",
    "\n",
    "t_train_append = np.append(t_train_sample,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "\n",
    "t_train_append.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d6e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(t_train_append.shape[0]):\n",
    "#    print(t_train_append[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44fdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_augmented_dataset(dataset):\n",
    "    augmented_dataset =data_augmentation(dataset)\n",
    "    augmented_dataset_numpy = augmented_dataset.numpy()\n",
    "    return augmented_dataset_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b484e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset1 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6269ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000018D6240D0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000018D61921E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset2 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37acbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset3 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2203cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset4 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb12889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(X_train_reshaped,augmented_dataset1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ade268",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966d0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5a5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4923ef7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170, 300, 300, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2db0d8",
   "metadata": {},
   "source": [
    "# Importing / Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f48472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "X_train_augmented = np.load('X_train_augmented.npy')\n",
    "t_train_augmented = np.load('t_train_augmented.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e68e59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5406ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a358ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634e5cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5, 5, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3460d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ca3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9f831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = tf.keras.Sequential()\n",
    "model_seq.add(keras.layers.Dropout(0.25))\n",
    "model_seq.add(base_model)\n",
    "\n",
    "#model_seq.add(keras.layers.Flatten())\n",
    "model_seq.add(keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(512, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(256, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "#model_seq.add(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83438a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = model_seq(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9aa74bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Option 1: Pooling\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert features of shape `base_model.output_shape[1:]` to vectors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_pooling \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m x_pooling\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# Option 1: Pooling\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_pooling = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acb0c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee96a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faaee81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "631/631 [==============================] - 432s 678ms/step - loss: 0.5593 - accuracy: 0.8177 - val_loss: 0.2841 - val_accuracy: 0.9118\n",
      "Epoch 2/15\n",
      "631/631 [==============================] - 401s 635ms/step - loss: 0.2630 - accuracy: 0.9097 - val_loss: 0.3734 - val_accuracy: 0.8989\n",
      "Epoch 3/15\n",
      "631/631 [==============================] - 406s 643ms/step - loss: 0.1777 - accuracy: 0.9411 - val_loss: 0.1823 - val_accuracy: 0.9504\n",
      "Epoch 4/15\n",
      "631/631 [==============================] - 397s 629ms/step - loss: 0.1200 - accuracy: 0.9595 - val_loss: 0.2792 - val_accuracy: 0.9405\n",
      "Epoch 5/15\n",
      "631/631 [==============================] - 403s 639ms/step - loss: 0.0945 - accuracy: 0.9682 - val_loss: 0.2525 - val_accuracy: 0.9485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaced3f880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_augmented,t_train_augmented, epochs=15, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4694b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 16s 577ms/step - loss: 0.1197 - accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11972300708293915, 0.9674157500267029]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa882836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 18s 601ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58f3744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       0.96      0.98      0.97        90\n",
      "       Adidas       0.99      0.95      0.97        88\n",
      "         Ford       0.99      0.98      0.98        88\n",
      "        Honda       0.98      0.99      0.98        88\n",
      "General_mills       0.98      0.98      0.98        90\n",
      "     Unilever       0.96      1.00      0.98        91\n",
      "    Mcdonalds       0.98      0.94      0.96        88\n",
      "          KFC       0.99      0.92      0.95        88\n",
      "       Gators       0.91      0.97      0.94        88\n",
      "           3M       0.96      0.97      0.96        91\n",
      "\n",
      "     accuracy                           0.97       890\n",
      "    macro avg       0.97      0.97      0.97       890\n",
      " weighted avg       0.97      0.97      0.97       890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d930aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd8328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
