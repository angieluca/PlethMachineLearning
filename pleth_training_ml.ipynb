{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bd0f13",
   "metadata": {},
   "source": [
    "# Final Thesis Project - Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837813a4",
   "metadata": {},
   "source": [
    "This Notebook tests out different ML models and check the scores. \n",
    "\n",
    "The training dataset contains a total of ? samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eae16b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e246e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edward.luca\\Github\\PlethMachineLearning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5423a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2101, 27) (2101,)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "data_train = np.load('data_train.npy', allow_pickle=True)\n",
    "labels_train = np.load('labels_train.npy', allow_pickle=True)\n",
    "\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00e819c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Encoding\n",
    "\n",
    "labels_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf4e55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting number samples per class\n",
    "vals, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(vals, counts)\n",
    "plt.xticks(range(7),range(7))\n",
    "plt.xlabel('Classes',size=20)\n",
    "plt.ylabel('# Samples per Class', size=20)\n",
    "plt.title('Training Data (Total = '+str(data_train.shape[0])+' samples)',size=15);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c11a0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from skimage.transform import resize\n",
    "from sklearn.svm import SVC\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc8d9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1680, 27)\n",
      "(1680,)\n",
      "(421, 27)\n",
      "(421,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(data_train, labels_train, \n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=labels_train,\n",
    "                                                   random_state=0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078ad78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f192e",
   "metadata": {},
   "source": [
    "## 1.) LDA + LOGISTIC REGRESSION (Model No.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a45a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edward.luca\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()),\n",
       "                ('LDA', LinearDiscriminantAnalysis(n_components=4)),\n",
       "                ('LOGRES', LogisticRegression())])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('LDA', LDA(n_components=4)),\n",
    "                 ('LOGRES', LogisticRegression())])\n",
    "mod1.fit(X_train, t_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0bb31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################   RUN GRIDSEARCHCV ON ALL    ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "880dc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1 = mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e0e1025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\n",
      "Accuracy:\n",
      " 0.9643705463182898\n",
      "F1_score:\n",
      " [0.98263889 1.         0.97797357 0.         0.58823529]\n",
      "Confusion matrix:\n",
      " [[283   0   0   0   2]\n",
      " [  0   2   0   0   0]\n",
      " [  1   0 111   0   0]\n",
      " [  0   0   0   0   1]\n",
      " [  7   0   4   0  10]]\n"
     ]
    }
   ],
   "source": [
    "print('LR\\n')\n",
    "print('Accuracy:\\n',accuracy_score(t_test, pred_test1))\n",
    "print ('F1_score:\\n',f1_score(t_test, pred_test1, average=None))\n",
    "print('Confusion matrix:\\n',confusion_matrix(t_test, pred_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2138975",
   "metadata": {},
   "source": [
    "## 2.) PCA + LOGISTIC REGRESSION (Model No. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57db1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64),)\n",
      "0.9999999999934971\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, D = np.shape(X_train)\n",
    "pca = PCA(n_components=min(N,D))\n",
    "pca.fit(X_train)\n",
    "\n",
    "plt.step(range(1,min(N,D)+1),np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "print(np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9))\n",
    "print(np.cumsum(pca.explained_variance_ratio_)[20])\n",
    "plt.xlabel('Number of principal components');\n",
    "plt.ylabel('% Variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "771eb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = Pipeline([('SCALER', StandardScaler()),\n",
    "                 ('PCA', PCA(n_components=26)),\n",
    "                 ('LOGREG', LogisticRegression(random_state=0, tol=0.01))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e30adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edward.luca\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('SCALER', StandardScaler()), ('PCA', PCA(n_components=26)),\n",
       "                ('LOGREG', LogisticRegression(random_state=0, tol=0.01))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5997c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With PCA:\n",
      "Test Accuracy Score =  0.9809976247030879\n",
      "Confusion matrix:\n",
      "[[284   0   0   0   1]\n",
      " [  0   2   0   0   0]\n",
      " [  1   0 111   0   0]\n",
      " [  1   0   0   0   0]\n",
      " [  2   0   3   0  16]]\n"
     ]
    }
   ],
   "source": [
    "pred_test2 = mod2.predict(X_test)\n",
    "\n",
    "print('With PCA:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test2))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf63d4",
   "metadata": {},
   "source": [
    "## 3.) Random Forest (Model No. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5624b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49ed5472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "819bf8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Random Forest:\n",
      "Test Accuracy Score =  0.9738717339667459\n",
      "Confusion matrix:\n",
      "[[281   0   0   0   4]\n",
      " [  0   2   0   0   0]\n",
      " [  0   0 111   0   1]\n",
      " [  0   0   0   0   1]\n",
      " [  3   0   2   0  16]]\n"
     ]
    }
   ],
   "source": [
    "pred_test3 = rf_classifier.predict(X_test)\n",
    "\n",
    "print('With Random Forest:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a5db9",
   "metadata": {},
   "source": [
    "## 4.) XGBoost (Model No.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a654a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8a8000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce01505a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_class=5,\n",
       "              num_parallel_tree=None, objective='multi:softmax', ...)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "xgb_classifier.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eeb1e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With XGBoost:\n",
      "Test Accuracy Score =  0.9714964370546318\n",
      "Confusion matrix:\n",
      "[[280   0   0   0   5]\n",
      " [  0   2   0   0   0]\n",
      " [  0   0 111   0   1]\n",
      " [  0   0   0   0   1]\n",
      " [  3   0   2   0  16]]\n"
     ]
    }
   ],
   "source": [
    "pred_test4 = xgb_classifier.predict(X_test)\n",
    "\n",
    "print('With XGBoost:')\n",
    "print('Test Accuracy Score = ',accuracy_score(t_test, pred_test4))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(t_test, pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd81d4",
   "metadata": {},
   "source": [
    "## 5.) CNN (Model No. 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b124395f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3d1e6d42ad48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f70bf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e49996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39d4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_full, t_train_full\n",
    "# free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017d5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a817652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=[300,300,3]), \n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'), \n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.MaxPooling2D(2), \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb48668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "127/127 [==============================] - 329s 3s/step - loss: 7.6946 - accuracy: 0.1004 - val_loss: 2.3032 - val_accuracy: 0.0991\n",
      "Epoch 2/2\n",
      "127/127 [==============================] - 324s 3s/step - loss: 2.3043 - accuracy: 0.0910 - val_loss: 2.3029 - val_accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c374bc5970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train, epochs=2, batch_size=32,\n",
    "          validation_data=(X_val, t_val),\n",
    "         callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71e240b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 13s 477ms/step - loss: 2.3029 - accuracy: 0.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3029372692108154, 0.10112359374761581]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d920484",
   "metadata": {},
   "source": [
    "## 6) Pre-trained CNN Model Using ResNet without Regularization (Model No. 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9edc452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f1ebde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0607fcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477fc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24905076",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45460de0",
   "metadata": {},
   "source": [
    "nn.Linear(numFeatures, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, len(trainDS.classes))\n",
    "        # nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff85d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a794e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f1c5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = base_model(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ef19353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f16ecefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 51200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e424e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dense classifier with 10 units and softmax activation function\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3660963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f49b217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 300, 300, 3]),\n",
       " TensorShape([None, 150, 150, 3]),\n",
       " TensorShape([None, 5, 5, 2048]),\n",
       " TensorShape([None, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs_resized.shape, x.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55cad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "127/127 [==============================] - 112s 842ms/step - loss: 643.4494 - accuracy: 0.3265 - val_loss: 1053.8110 - val_accuracy: 0.2319\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 111s 877ms/step - loss: 403.5866 - accuracy: 0.4958 - val_loss: 1754.6864 - val_accuracy: 0.1903\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 107s 841ms/step - loss: 329.6902 - accuracy: 0.5885 - val_loss: 927.3879 - val_accuracy: 0.2963\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 236.7023 - accuracy: 0.6490 - val_loss: 561.5453 - val_accuracy: 0.4281\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 106s 839ms/step - loss: 193.1102 - accuracy: 0.7035 - val_loss: 3759.5913 - val_accuracy: 0.1308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c49bc9f850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_reshaped,t_train, epochs=5, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "facf5d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 20s 707ms/step - loss: 3681.6174 - accuracy: 0.1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3681.617431640625, 0.13370786607265472]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa4fc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 21s 713ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a0e9153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       1.00      0.02      0.04        90\n",
      "       Adidas       0.00      0.00      0.00        88\n",
      "         Ford       0.86      0.07      0.13        88\n",
      "        Honda       0.67      0.09      0.16        88\n",
      "General Mills       0.00      0.00      0.00        90\n",
      "     Unilever       1.00      0.01      0.02        91\n",
      "   McDonald's       0.10      1.00      0.19        88\n",
      "          KFC       1.00      0.02      0.04        88\n",
      "       Gators       0.86      0.14      0.24        88\n",
      "           3M       0.00      0.00      0.00        91\n",
      "\n",
      "     accuracy                           0.13       890\n",
      "    macro avg       0.55      0.14      0.08       890\n",
      " weighted avg       0.55      0.13      0.08       890\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\renii\\.conda\\envs\\EEE4773\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca67ff1",
   "metadata": {},
   "source": [
    "## 7) Pre-trained CNN Model Using ResNet with Regularization (Model No.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace4417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.utils.set_random_seed(\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746adea",
   "metadata": {},
   "source": [
    "###  Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb2935",
   "metadata": {},
   "source": [
    "Tutorial on Data Augmentation:\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d9dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, label):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title(label)\n",
    "  plt.imshow(image/255.0)\n",
    "    \n",
    "    \n",
    "\n",
    "def visualize_both(original, augmented):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title('Original image')\n",
    "  plt.imshow(original)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.title('Augmented image')\n",
    "  plt.imshow(augmented/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d61d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_train.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951d4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  keras.layers.RandomFlip(\"horizontal\"),\n",
    "  keras.layers.RandomRotation(0.2),\n",
    "  keras.layers.RandomBrightness(0.3),\n",
    "  keras.layers.RandomContrast(0.4),\n",
    "  #keras.layers.RandomCrop(height=0.5,width=0.5,seed=0),\n",
    "  keras.layers.RandomZoom(height_factor=0.5,width_factor=0.5,seed=0),\n",
    "  #keras.layers.RandomWidth(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135deedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sample = X_train_reshaped\n",
    "t_train_sample = t_train\n",
    "\n",
    "#X_train_sample = X_train_reshaped[0,:,:,:]\n",
    "#t_train_sample = t_train[0]\n",
    "\n",
    "t_train_append = np.append(t_train_sample,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "t_train_append = np.append(t_train_append,t_train_sample)\n",
    "\n",
    "t_train_append.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d6e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(t_train_append.shape[0]):\n",
    "#    print(t_train_append[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44fdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_augmented_dataset(dataset):\n",
    "    augmented_dataset =data_augmentation(dataset)\n",
    "    augmented_dataset_numpy = augmented_dataset.numpy()\n",
    "    return augmented_dataset_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b484e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset1 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6269ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000018D6240D0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000018D61921E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset2 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37acbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset3 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2203cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset4 = return_augmented_dataset(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb12889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(X_train_reshaped,augmented_dataset1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ade268",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966d0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5a5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = np.append(augmented_dataset,augmented_dataset4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4923ef7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20170, 300, 300, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2db0d8",
   "metadata": {},
   "source": [
    "# Importing / Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f48472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 270000), (5933,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = np.load('data_train.npy').T\n",
    "t_train_full = np.load('labels_train_corrected.npy')\n",
    "X_train_augmented = np.load('X_train_augmented.npy')\n",
    "t_train_augmented = np.load('t_train_augmented.npy')\n",
    "\n",
    "X_train_full.shape, t_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e68e59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 270000), (5043,), (4034, 270000), (4034,), (1009, 270000), (1009,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test sets\n",
    "X_training, X_test, t_training, t_test = train_test_split(X_train_full, \n",
    "                                                  t_train_full, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_train_full,\n",
    "                                                  test_size=0.15)\n",
    "# Train and validation sets\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_training, \n",
    "                                                  t_training, \n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=t_training,\n",
    "                                                  test_size=0.2)\n",
    "\n",
    "X_training.shape, t_training.shape, X_train.shape, t_train.shape, X_val.shape, t_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5406ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_training = X_training.reshape(X_training.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_val = X_val.reshape(X_val.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "#X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)/255.0\n",
    "\n",
    "X_training_reshaped = X_training.reshape(-1, 300, 300, 3)\n",
    "X_test_reshaped = X_test.reshape(-1, 300, 300, 3)\n",
    "\n",
    "# Reshape the input data to match the model's expected input shape\n",
    "X_train_reshaped = X_train.reshape(-1, 300, 300, 3)\n",
    "X_val_reshaped = X_val.reshape(-1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a358ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634e5cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5, 5, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3460d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ca3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9f831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = tf.keras.Sequential()\n",
    "model_seq.add(keras.layers.Dropout(0.25))\n",
    "model_seq.add(base_model)\n",
    "\n",
    "#model_seq.add(keras.layers.Flatten())\n",
    "model_seq.add(keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(512, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(256, activation='relu'))\n",
    "model_seq.add(keras.layers.Dropout(0.50))\n",
    "model_seq.add(keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "#model_seq.add(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83438a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "\n",
    "# .Input() instantiates a Keras tensor\n",
    "inputs = keras.Input(shape=(300, 300, 3))\n",
    "# Input layer\n",
    "\n",
    "inputs_resized = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)(inputs)\n",
    "# resizing input to match pretrained model\n",
    "\n",
    "x = model_seq(inputs_resized, training=False)\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9aa74bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Option 1: Pooling\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert features of shape `base_model.output_shape[1:]` to vectors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_pooling \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m x_pooling\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\EEE4773\\lib\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d_1\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# Option 1: Pooling\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_pooling = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acb0c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Flattening\n",
    "\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x_flatten = keras.layers.Flatten()(x)\n",
    "\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee96a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(10, activation='softmax')(x_flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faaee81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "631/631 [==============================] - 432s 678ms/step - loss: 0.5593 - accuracy: 0.8177 - val_loss: 0.2841 - val_accuracy: 0.9118\n",
      "Epoch 2/15\n",
      "631/631 [==============================] - 401s 635ms/step - loss: 0.2630 - accuracy: 0.9097 - val_loss: 0.3734 - val_accuracy: 0.8989\n",
      "Epoch 3/15\n",
      "631/631 [==============================] - 406s 643ms/step - loss: 0.1777 - accuracy: 0.9411 - val_loss: 0.1823 - val_accuracy: 0.9504\n",
      "Epoch 4/15\n",
      "631/631 [==============================] - 397s 629ms/step - loss: 0.1200 - accuracy: 0.9595 - val_loss: 0.2792 - val_accuracy: 0.9405\n",
      "Epoch 5/15\n",
      "631/631 [==============================] - 403s 639ms/step - loss: 0.0945 - accuracy: 0.9682 - val_loss: 0.2525 - val_accuracy: 0.9485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaced3f880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_augmented,t_train_augmented, epochs=15, batch_size=32,\n",
    "          validation_data=(X_val_reshaped, t_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)])\n",
    "\n",
    "# Again, in practice, you would run for a lot more epochs. \n",
    "# As well as perform the necessary hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4694b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 16s 577ms/step - loss: 0.1197 - accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11972300708293915, 0.9674157500267029]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa882836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 18s 601ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label predictions\n",
    "y_test = np.argmax(model.predict(X_test_reshaped),axis=1)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58f3744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Nike       0.96      0.98      0.97        90\n",
      "       Adidas       0.99      0.95      0.97        88\n",
      "         Ford       0.99      0.98      0.98        88\n",
      "        Honda       0.98      0.99      0.98        88\n",
      "General_mills       0.98      0.98      0.98        90\n",
      "     Unilever       0.96      1.00      0.98        91\n",
      "    Mcdonalds       0.98      0.94      0.96        88\n",
      "          KFC       0.99      0.92      0.95        88\n",
      "       Gators       0.91      0.97      0.94        88\n",
      "           3M       0.96      0.97      0.96        91\n",
      "\n",
      "     accuracy                           0.97       890\n",
      "    macro avg       0.97      0.97      0.97       890\n",
      " weighted avg       0.97      0.97      0.97       890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_test, y_test, target_names=labels_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d930aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd8328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
